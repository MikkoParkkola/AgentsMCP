#!/usr/bin/env python3
"""
orchestrator_monitor.py

A production‑ready monitoring tool for the AgentsMCP distributed orchestrator
system.  It can run continuously or once to produce a performance report.

Features
--------
1. Monitor orchestrator response times, token usage, and cost per task.
2. Track worker availability, health, and task queue depth.
3. Compare different orchestrator models (e.g. GPT‑4, GPT‑3.5).
4. Export collected metrics in JSON format.
5. CLI interface with argparse, configurable log level, output path, interval,
   and mode (continuous / one‑shot).
6. Robust error handling and structured logging.

Dependencies
------------
* Python 3.8+
* `requests` (install with `pip install requests`)

Usage
-----
    # One‑shot report
    python orchestrator_monitor.py --once \
        --models gpt-5,claude-4.1-sonnet \
        --output report.json

    # Continuous monitoring, 30‑second interval
    python orchestrator_monitor.py --interval 30 \
        --models gpt-5,claude-4.1-sonnet \
        --log-level INFO

Author
------
Generated by Ollama gpt-oss:20b for AgentsMCP

"""

import argparse
import json
import logging
import sys
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

try:
    import requests
    from requests.exceptions import RequestException
except ImportError as exc:  # pragma: no cover
    print("Error: The 'requests' library is required. Install it with:\n    pip install requests", file=sys.stderr)
    raise exc

# --------------------------------------------------------------------------- #
# Configuration and Logging
# --------------------------------------------------------------------------- #

DEFAULT_LOG_LEVEL = logging.INFO
DEFAULT_INTERVAL = 60  # seconds
DEFAULT_BASE_URL = "http://localhost:8000"  # Default API endpoint


def setup_logging(level: int) -> None:
    """Configure root logger."""
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


# --------------------------------------------------------------------------- #
# Data Structures
# --------------------------------------------------------------------------- #

@dataclass
class OrchestratorMetric:
    model: str
    response_time_ms: float
    tokens_input: int
    tokens_output: int
    cost_per_task: float
    timestamp: str

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class WorkerMetric:
    worker_id: str
    available: bool
    health_status: str
    active_tasks: int
    timestamp: str

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class PerformanceReport:
    orchestrator_metrics: List[OrchestratorMetric]
    worker_metrics: List[WorkerMetric]
    queue_depth: int
    success_rate: float
    failure_rate: float
    timestamp: str

    def to_dict(self) -> Dict:
        return {
            "timestamp": self.timestamp,
            "queue_depth": self.queue_depth,
            "success_rate": self.success_rate,
            "failure_rate": self.failure_rate,
            "orchestrator_metrics": [m.to_dict() for m in self.orchestrator_metrics],
            "worker_metrics": [m.to_dict() for m in self.worker_metrics],
        }


# --------------------------------------------------------------------------- #
# Utility Functions
# --------------------------------------------------------------------------- #

def _current_timestamp() -> str:
    return datetime.utcnow().isoformat() + "Z"


def _safe_request(url: str, timeout: int = 10) -> Optional[Dict]:
    """Perform a GET request and return JSON or None on error."""
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()
        return response.json()
    except RequestException as exc:
        logging.error("Request to %s failed: %s", url, exc)
    return None


def _compute_cost(tokens_input: int, tokens_output: int, cost_per_token: float) -> float:
    return (tokens_input + tokens_output) * cost_per_token


# --------------------------------------------------------------------------- #
# API Interaction (Replace with real endpoints)
# --------------------------------------------------------------------------- #

def fetch_orchestrator_metrics(
    base_url: str, model: str, cost_per_token: float
) -> Optional[OrchestratorMetric]:
    """
    Query the orchestrator for metrics.

    Expected JSON structure from API (example):
    {
        "response_time_ms": 125.6,
        "tokens_input": 1500,
        "tokens_output": 2000,
        "success": true,
        "failure": false
    }
    """
    url = f"{base_url}/orchestrator/{model}/metrics"
    data = _safe_request(url)
    if not data:
        return None

    try:
        response_time_ms = float(data["response_time_ms"])
        tokens_input = int(data["tokens_input"])
        tokens_output = int(data["tokens_output"])
        cost = _compute_cost(tokens_input, tokens_output, cost_per_token)
        return OrchestratorMetric(
            model=model,
            response_time_ms=response_time_ms,
            tokens_input=tokens_input,
            tokens_output=tokens_output,
            cost_per_task=cost,
            timestamp=_current_timestamp(),
        )
    except (KeyError, ValueError, TypeError) as exc:
        logging.error("Malformed orchestrator metric data for model %s: %s", model, exc)
        return None


def fetch_worker_metrics(base_url: str) -> List[WorkerMetric]:
    """
    Query all workers for their status.

    Expected JSON structure from API (example):
    [
        {
            "worker_id": "worker-01",
            "available": true,
            "health_status": "healthy",
            "active_tasks": 0
        },
        ...
    ]
    """
    url = f"{base_url}/workers/status"
    data = _safe_request(url)
    if not isinstance(data, list):
        logging.error("Worker metrics endpoint returned invalid data: %s", data)
        return []

    metrics = []
    for item in data:
        try:
            metrics.append(
                WorkerMetric(
                    worker_id=str(item["worker_id"]),
                    available=bool(item["available"]),
                    health_status=str(item["health_status"]),
                    active_tasks=int(item["active_tasks"]),
                    timestamp=_current_timestamp(),
                )
            )
        except (KeyError, ValueError, TypeError) as exc:
            logging.warning("Skipping malformed worker entry: %s", exc)
    return metrics


def fetch_task_queue_depth(base_url: str) -> int:
    """
    Query the task queue depth.

    Expected JSON structure from API (example):
    { "queue_depth": 42 }
    """
    url = f"{base_url}/queue/depth"
    data = _safe_request(url)
    if data and isinstance(data.get("queue_depth"), int):
        return data["queue_depth"]
    logging.warning("Failed to retrieve queue depth; defaulting to 0")
    return 0


def fetch_success_failure_rates(base_url: str) -> (float, float):
    """
    Query overall success/failure rates.

    Expected JSON structure from API (example):
    { "success_rate": 0.97, "failure_rate": 0.03 }
    """
    url = f"{base_url}/metrics/success_rates"
    data = _safe_request(url)
    if data:
        try:
            return float(data["success_rate"]), float(data["failure_rate"])
        except (KeyError, ValueError, TypeError) as exc:
            logging.warning("Malformed success rate data: %s", exc)
    # Default to 100% success if endpoint is unavailable
    return 1.0, 0.0


# --------------------------------------------------------------------------- #
# Report Generation
# --------------------------------------------------------------------------- #

def generate_performance_report(
    base_url: str,
    models: List[str],
    cost_per_token: float,
) -> PerformanceReport:
    orchestrator_metrics: List[OrchestratorMetric] = []
    for model in models:
        metric = fetch_orchestrator_metrics(base_url, model, cost_per_token)
        if metric:
            orchestrator_metrics.append(metric)

    worker_metrics = fetch_worker_metrics(base_url)
    queue_depth = fetch_task_queue_depth(base_url)
    success_rate, failure_rate = fetch_success_failure_rates(base_url)

    report = PerformanceReport(
        orchestrator_metrics=orchestrator_metrics,
        worker_metrics=worker_metrics,
        queue_depth=queue_depth,
        success_rate=success_rate,
        failure_rate=failure_rate,
        timestamp=_current_timestamp(),
    )
    return report


def export_report_to_json(report: PerformanceReport, output_path: Optional[Path]) -> None:
    json_data = json.dumps(report.to_dict(), indent=4)
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(json_data, encoding="utf-8")
        logging.info("Report written to %s", output_path)
    else:
        print(json_data)


def print_report(report: PerformanceReport) -> None:
    """Pretty‑print a human‑readable report to stdout."""
    print(f"\n===== Performance Report ({report.timestamp}) =====")
    print(f"Task Queue Depth: {report.queue_depth}")
    print(f"Success Rate: {report.success_rate:.2%} | Failure Rate: {report.failure_rate:.2%}")
    print("\nOrchestrator Metrics:")
    for m in report.orchestrator_metrics:
        print(
            f"  - Model: {m.model}\n"
            f"    Response Time: {m.response_time_ms:.1f} ms\n"
            f"    Tokens: {m.tokens_input} in / {m.tokens_output} out\n"
            f"    Cost per Task: ${m.cost_per_task:.4f}\n"
        )
    print("\nWorker Status:")
    for w in report.worker_metrics:
        status = "Available" if w.available else "Busy"
        print(
            f"  - {w.worker_id}: {status} | Health: {w.health_status} | Active Tasks: {w.active_tasks}"
        )
    print("===================================================\n")


# --------------------------------------------------------------------------- #
# CLI Handling
# --------------------------------------------------------------------------- #

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Monitor AgentsMCP orchestrator and workers, generate performance reports."
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--once",
        action="store_true",
        help="Run once and exit (default is continuous mode)",
    )
    group.add_argument(
        "--interval",
        type=int,
        default=DEFAULT_INTERVAL,
        help="Seconds between each report in continuous mode (default: 60)",
    )
    parser.add_argument(
        "--models",
        type=str,
        default="gpt-5,claude-4.1-sonnet",
        help="Comma‑separated list of orchestrator models to monitor",
    )
    parser.add_argument(
        "--base-url",
        type=str,
        default=DEFAULT_BASE_URL,
        help=f"Base URL for the orchestrator API (default: {DEFAULT_BASE_URL})",
    )
    parser.add_argument(
        "--cost-per-token",
        type=float,
        default=0.00002,
        help="Cost per token in USD for computing cost per task",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="File path to write JSON report; if omitted, prints to stdout",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level",
    )
    return parser.parse_args()


# --------------------------------------------------------------------------- #
# Main Loop
# --------------------------------------------------------------------------- #

def main() -> None:
    args = parse_args()
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    setup_logging(log_level)

    models = [m.strip() for m in args.models.split(",") if m.strip()]
    if not models:
        logging.error("No valid models specified.")
        sys.exit(1)

    output_path = Path(args.output) if args.output else None

    if args.once:
        logging.info("Running one‑shot monitoring.")
        report = generate_performance_report(
            base_url=args.base_url,
            models=models,
            cost_per_token=args.cost_per_token,
        )
        print_report(report)
        export_report_to_json(report, output_path)
    else:
        interval = max(5, args.interval)  # Enforce a minimum interval
        logging.info("Starting continuous monitoring every %s seconds.", interval)
        try:
            while True:
                report = generate_performance_report(
                    base_url=args.base_url,
                    models=models,
                    cost_per_token=args.cost_per_token,
                )
                print_report(report)
                export_report_to_json(report, output_path)
                time.sleep(interval)
        except KeyboardInterrupt:
            logging.info("Monitoring stopped by user.")
            sys.exit(0)


if __name__ == "__main__":
    main()